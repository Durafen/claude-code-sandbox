#!/bin/bash

# Sandboxed Claude Code CLI wrapper
# Based on https://github.com/cwensel/claude-sandbox/blob/main/claudes

# Function to generate random suffix
generate_random_suffix() {
    python3 -c "import random, string; print(''.join(random.choices(string.ascii_lowercase + string.digits, k=8)))"
}

# Define base image name (shared across all workspaces)
BASE_IMAGE_NAME="claude-code-sandbox-base"

# Function to get content hash for change detection
get_content_hash() {
    {
        # Hash Dockerfile content
        cat "$1/Dockerfile" 2>/dev/null || echo "no-dockerfile"
        # Hash .tool-versions if present
        cat ".tool-versions" 2>/dev/null || echo "no-tool-versions" 
    } | sha256sum | cut -d' ' -f1 | head -c 8
}

# Determine image name based on local .claude-code-sandbox file or create new one
SANDBOX_FILE=".claude-code-sandbox"
if [ -f "$SANDBOX_FILE" ]; then
    IMAGE_NAME=$(cat "$SANDBOX_FILE")
    echo "Using existing workspace image: $IMAGE_NAME"
else
    # Generate new image name with random suffix
    RANDOM_SUFFIX=$(generate_random_suffix)
    IMAGE_NAME="claude-code-sandbox-${RANDOM_SUFFIX}"
    echo "$IMAGE_NAME" > "$SANDBOX_FILE"
    echo "Created new workspace image: $IMAGE_NAME"
fi

# Parse command line options
BUILD_IMAGE=false
SHELL_MODE=false
NON_INTERACTIVE=false
DOCKER_ENABLED=false
INSTALL_TOOLS=""
CLEANUP_MODE=false
CLEANUP_DAYS=7
DRY_RUN=false
REBUILD_IMAGE=false
REMOVE_IMAGE=false
SYNC_CONFIG=false
CLAUDE_MODE=true
GLM_MODE=false

while getopts "bsndcg-:" opt; do
    case $opt in
        b)
            BUILD_IMAGE=true
            ;;
        s)
            SHELL_MODE=true
            ;;
        n)
            NON_INTERACTIVE=true
            ;;
        d)
            DOCKER_ENABLED=true
            ;;
        c)
            CLAUDE_MODE=true
            SHELL_MODE=false
            ;;
        g)
            GLM_MODE=true
            CLAUDE_MODE=true
            SHELL_MODE=false
            ;;
        -)
            case "${OPTARG}" in
                build)
                    BUILD_IMAGE=true
                    ;;
                install=*)
                    INSTALL_TOOLS="${OPTARG#*=}"
                    ;;
                shell)
                    SHELL_MODE=true
                    ;;
                non-interactive)
                    NON_INTERACTIVE=true
                    ;;
                docker)
                    DOCKER_ENABLED=true
                    ;;
                claude)
                    CLAUDE_MODE=true
                    SHELL_MODE=false
                    ;;
                glm)
                    GLM_MODE=true
                    CLAUDE_MODE=true
                    SHELL_MODE=false
                    ;;
                cleanup)
                    CLEANUP_MODE=true
                    ;;
                sync-config)
                    SYNC_CONFIG=true
                    ;;
                older-than=*)
                    CLEANUP_DAYS="${OPTARG#*=}"
                    # Parse days from format like "7d", "3", "14d"
                    CLEANUP_DAYS="${CLEANUP_DAYS%d}"
                    ;;
                dry-run)
                    DRY_RUN=true
                    ;;
                rebuild)
                    REBUILD_IMAGE=true
                    BUILD_IMAGE=true
                    ;;
                remove)
                    REMOVE_IMAGE=true
                    ;;
                help)
                    echo "Claude Code Sandbox - Containerized development environment"
                    echo ""
                    echo "Usage: $0 [OPTIONS] [CLAUDE_ARGS...]"
                    echo ""
                    echo "Options:"
                    echo "  -b, --build                   Build/rebuild the Docker image"
                    echo "  -c, --claude                  Launch Claude Code (default)"
                    echo "  -g, --glm                     Launch Claude Code in GLM mode (z.ai proxy)"
                    echo "  -s, --shell                   Launch interactive shell"
                    echo "  -n, --non-interactive CMD     Run non-interactive command"
                    echo "  -d, --docker                  Enable Docker access inside container"
                    echo "      --install=TOOLS           Install tools at build time"
                    echo "      --rebuild                 Force full rebuild without cache"
                    echo "      --remove                  Remove workspace image and state files"
                    echo "      --cleanup                 Remove old workspace images (default: 7+ days)"
                    echo "      --sync-config             Sync host configs to sandbox (overwrites sandbox configs)"
                    echo "      --older-than=DAYS         Cleanup threshold (e.g. --older-than=3d)"
                    echo "      --dry-run                 Show what would be cleaned without doing it"
                    echo "      --help                    Show this help message"
                    echo ""
                    echo "Tool Installation:"
                    echo "  --install='tool1@version1,tool2@version2'"
                    echo ""
                    echo "Examples:"
                    echo "  $0 --build --install='python@3.12.8,golang@1.21.5'"
                    echo "  $0 --build --install='java@adoptopenjdk-17.0.2+8,terraform@1.5.7'"
                    echo "  $0 --rebuild                  # Force complete rebuild without cache"
                    echo "  $0 --remove                   # Remove current workspace image"
                    echo "  $0 --shell                    # Interactive shell"
                    echo "  $0 -n 'python --version'     # Non-interactive command"
                    echo "  $0 python --version          # Same as -n (positional args)"
                    echo "  $0 --cleanup                  # Remove images older than 7 days"
                    echo "  $0 --cleanup --older-than=3d  # Remove images older than 3 days"
                    echo "  $0 --cleanup --dry-run        # Preview cleanup without removing"
                    echo ""
                    echo "Auto-completion:"
                    echo "  Bash: source ./completions/claude-code-sandbox"
                    echo "  Zsh:  Add ./completions to your fpath and run: autoload -U compinit && compinit"
                    echo ""
                    echo "For more information, see: README.md"
                    exit 0
                    ;;
                *)
                    echo "Unknown option --${OPTARG}" >&2
                    echo "Usage: $0 --build --install='plugin1@version1,plugin2@version2'"
                    echo "Examples:"
                    echo "  $0 --build --install='python@3.12.8,golang@1.21.5,maven@3.9.6'"
                    echo "  $0 --build --install='java@adoptopenjdk-17.0.2+8,terraform@1.5.7'"
                    echo "  $0 --build --install='nodejs@20.11.0,rust@1.75.0'"
                    echo "Use --help for more information"
                    exit 1
                    ;;
            esac
            ;;
        \?)
            echo "Invalid option: -$OPTARG" >&2
            exit 1
            ;;
    esac
done

shift $((OPTIND-1))

# If positional arguments remain, treat them as a non-interactive command
if [[ $# -gt 0 ]]; then
    NON_INTERACTIVE=true
    SHELL_MODE=false
    set -- "$*"  # Combine all args into single command string
fi

# Function to perform image cleanup
cleanup_images() {
    local days=$1
    local dry_run=$2
    local current_image=$(cat "$SANDBOX_FILE" 2>/dev/null || echo "")
    
    echo "=== Claude Code Sandbox Image Cleanup ==="
    echo "Looking for workspace images older than $days days..."
    echo "Current workspace image: $current_image"
    echo "Base image (always preserved): $BASE_IMAGE_NAME"
    echo ""
    
    # Get current timestamp in seconds
    local current_time=$(date +%s)
    local cutoff_time=$((current_time - (days * 24 * 3600)))
    
    # Find all claude-code-sandbox workspace images (exclude base)
    local images_to_check=$(docker images --format "{{.Repository}}\t{{.Tag}}\t{{.CreatedAt}}\t{{.Size}}\t{{.ID}}" | \
        grep "^claude-code-sandbox-" | grep -v "^$BASE_IMAGE_NAME")
    
    if [[ -z "$images_to_check" ]]; then
        echo "No workspace images found to clean up."
        return 0
    fi
    
    local images_to_remove=()
    local total_size=0
    
    echo "Scanning workspace images:"
    echo "Repository                        Created                Size      Image ID       Status"
    echo "----------------------------------------------------------------------------------------"
    
    # Process each image
    while IFS=$'\t' read -r repo tag created_at size image_id; do
        if [[ -z "$repo" ]]; then continue; fi
        
        # Use repo name directly (it should be the image name)
        local image_name="$repo"
        
        # Skip if this is the current workspace image
        if [[ "$image_name" == "$current_image" ]]; then
            printf "%-32s %-22s %-9s %-14s %s\n" "$repo" "$created_at" "$size" "$image_id" "(CURRENT - PRESERVED)"
            continue
        fi
        
        # Parse creation time - remove timezone for compatibility
        local clean_date=$(echo "$created_at" | sed 's/ -[0-9]* [A-Z]*$//')
        local image_time
        if command -v gdate >/dev/null 2>&1; then
            # macOS with coreutils
            image_time=$(gdate -d "$clean_date" +%s 2>/dev/null || echo 0)
        else
            # Linux date
            image_time=$(date -d "$clean_date" +%s 2>/dev/null || echo 0)
        fi
        
        if [[ $image_time -lt $cutoff_time ]]; then
            printf "%-32s %-22s %-9s %-14s %s\n" "$repo" "$created_at" "$size" "$image_id" "(MARKED FOR REMOVAL)"
            images_to_remove+=("$image_name")
            # Convert size to bytes for summing (rough estimate)
            local size_mb=$(echo "$size" | sed 's/[^0-9.]//g')
            if [[ -n "$size_mb" && "$size_mb" != "" ]]; then
                total_size=$((total_size + ${size_mb%.*}))
            fi
        else
            printf "%-32s %-22s %-9s %-14s %s\n" "$repo" "$created_at" "$size" "$image_id" "(TOO RECENT - PRESERVED)"
        fi
    done <<< "$images_to_check"
    
    echo ""
    
    if [[ ${#images_to_remove[@]} -eq 0 ]]; then
        echo "‚úÖ No images need cleanup (all are either current workspace, base image, or too recent)"
        return 0
    fi
    
    echo "üìã Summary:"
    echo "  Images to remove: ${#images_to_remove[@]}"
    echo "  Estimated space to reclaim: ~${total_size}MB"
    echo ""
    
    if [[ "$dry_run" == "true" ]]; then
        echo "üîç DRY RUN - No images will actually be removed"
        echo "Images that would be removed:"
        for image in "${images_to_remove[@]}"; do
            echo "  - $image"
        done
        return 0
    fi
    
    # Confirm removal
    echo "‚ö†Ô∏è  This will permanently remove ${#images_to_remove[@]} Docker images."
    echo "Are you sure you want to continue? (y/N)"
    read -r confirmation
    
    if [[ ! "$confirmation" =~ ^[Yy]$ ]]; then
        echo "‚ùå Cleanup cancelled"
        return 0
    fi
    
    echo ""
    echo "üóëÔ∏è  Removing images..."
    
    local removed_count=0
    for image in "${images_to_remove[@]}"; do
        echo "Removing: $image"
        if docker rmi "$image" >/dev/null 2>&1; then
            echo "  ‚úÖ Removed successfully"
            ((removed_count++))
        else
            echo "  ‚ö†Ô∏è  Failed to remove (might be in use)"
        fi
    done
    
    echo ""
    echo "‚úÖ Cleanup complete!"
    echo "  Successfully removed: $removed_count/${#images_to_remove[@]} images"
    
    if [[ $removed_count -gt 0 ]]; then
        echo ""
        echo "üí° Consider running 'docker system prune' to reclaim additional space from dangling images and build cache"
    fi
}

# Handle cleanup mode
if [[ "$CLEANUP_MODE" == true ]]; then
    cleanup_images "$CLEANUP_DAYS" "$DRY_RUN"
    exit 0
fi

# Sandbox config directory (persistent, protects host files)
SANDBOX_CONFIG_DIR="$HOME/.claude-sandbox"

# Function to sync host configs to sandbox (force mode overwrites existing)
sync_sandbox_config() {
    local force=${1:-true}  # Default to force mode for --sync-config
    echo "=== Syncing host configs to sandbox ==="
    mkdir -p "$SANDBOX_CONFIG_DIR"

    # Sync .claude directory (skip broken symlinks and local npm install)
    if [ -d "$HOME/.claude" ]; then
        if [ "$force" = true ] || [ ! -d "$SANDBOX_CONFIG_DIR/.claude" ]; then
            echo "Syncing ~/.claude ..."
            rm -rf "$SANDBOX_CONFIG_DIR/.claude"
            mkdir -p "$SANDBOX_CONFIG_DIR/.claude"
            rsync -a --ignore-errors --exclude='local' "$HOME/.claude/" "$SANDBOX_CONFIG_DIR/.claude/" 2>/dev/null
        fi
    fi

    # Sync .claude.json
    if [ -f "$HOME/.claude.json" ]; then
        if [ "$force" = true ] || [ ! -f "$SANDBOX_CONFIG_DIR/.claude.json" ]; then
            echo "Syncing ~/.claude.json ..."
            cp "$HOME/.claude.json" "$SANDBOX_CONFIG_DIR/.claude.json"
        fi
    fi

    # Sync .config/claude
    if [ -d "$HOME/.config/claude" ]; then
        if [ "$force" = true ] || [ ! -d "$SANDBOX_CONFIG_DIR/.config/claude" ]; then
            echo "Syncing ~/.config/claude ..."
            mkdir -p "$SANDBOX_CONFIG_DIR/.config"
            rm -rf "$SANDBOX_CONFIG_DIR/.config/claude"
            cp -r "$HOME/.config/claude" "$SANDBOX_CONFIG_DIR/.config/claude"
        fi
    fi

    # Sync .anthropic
    if [ -d "$HOME/.anthropic" ]; then
        if [ "$force" = true ] || [ ! -d "$SANDBOX_CONFIG_DIR/.anthropic" ]; then
            echo "Syncing ~/.anthropic ..."
            rm -rf "$SANDBOX_CONFIG_DIR/.anthropic"
            cp -r "$HOME/.anthropic" "$SANDBOX_CONFIG_DIR/.anthropic"
        fi
    fi

    # Sync .config/opencode
    if [ -d "$HOME/.config/opencode" ]; then
        if [ "$force" = true ] || [ ! -d "$SANDBOX_CONFIG_DIR/.config/opencode" ]; then
            echo "Syncing ~/.config/opencode ..."
            mkdir -p "$SANDBOX_CONFIG_DIR/.config"
            rm -rf "$SANDBOX_CONFIG_DIR/.config/opencode"
            cp -r "$HOME/.config/opencode" "$SANDBOX_CONFIG_DIR/.config/opencode"
        fi
    fi

    # Sync .local/share/opencode
    if [ -d "$HOME/.local/share/opencode" ]; then
        if [ "$force" = true ] || [ ! -d "$SANDBOX_CONFIG_DIR/.local/share/opencode" ]; then
            echo "Syncing ~/.local/share/opencode ..."
            mkdir -p "$SANDBOX_CONFIG_DIR/.local/share"
            rm -rf "$SANDBOX_CONFIG_DIR/.local/share/opencode"
            cp -r "$HOME/.local/share/opencode" "$SANDBOX_CONFIG_DIR/.local/share/opencode"
        fi
    fi

    # Sync .cache/opencode (isolated from host cache)
    if [ -d "$HOME/.cache/opencode" ]; then
        if [ "$force" = true ] || [ ! -d "$SANDBOX_CONFIG_DIR/.cache/opencode" ]; then
            echo "Syncing ~/.cache/opencode ..."
            mkdir -p "$SANDBOX_CONFIG_DIR/.cache"
            rm -rf "$SANDBOX_CONFIG_DIR/.cache/opencode"
            cp -r "$HOME/.cache/opencode" "$SANDBOX_CONFIG_DIR/.cache/opencode"
        fi
    fi

    # Sync .local/state/opencode (model selection, favorites, history)
    if [ -d "$HOME/.local/state/opencode" ]; then
        if [ "$force" = true ] || [ ! -d "$SANDBOX_CONFIG_DIR/.local/state/opencode" ]; then
            echo "Syncing ~/.local/state/opencode ..."
            mkdir -p "$SANDBOX_CONFIG_DIR/.local/state"
            rm -rf "$SANDBOX_CONFIG_DIR/.local/state/opencode"
            cp -r "$HOME/.local/state/opencode" "$SANDBOX_CONFIG_DIR/.local/state/opencode"
        fi
    fi

    # Sync .qwen (Qwen Code)
    if [ -d "$HOME/.qwen" ]; then
        if [ "$force" = true ] || [ ! -d "$SANDBOX_CONFIG_DIR/.qwen" ]; then
            echo "Syncing ~/.qwen ..."
            rm -rf "$SANDBOX_CONFIG_DIR/.qwen"
            cp -r "$HOME/.qwen" "$SANDBOX_CONFIG_DIR/.qwen"
        fi
    fi

    # Sync .codex (OpenAI Codex CLI)
    if [ -d "$HOME/.codex" ]; then
        if [ "$force" = true ] || [ ! -d "$SANDBOX_CONFIG_DIR/.codex" ]; then
            echo "Syncing ~/.codex ..."
            rm -rf "$SANDBOX_CONFIG_DIR/.codex"
            cp -r "$HOME/.codex" "$SANDBOX_CONFIG_DIR/.codex"
        fi
    fi

    # Sync .gemini (Google Gemini CLI)
    if [ -d "$HOME/.gemini" ]; then
        if [ "$force" = true ] || [ ! -d "$SANDBOX_CONFIG_DIR/.gemini" ]; then
            echo "Syncing ~/.gemini ..."
            rm -rf "$SANDBOX_CONFIG_DIR/.gemini"
            cp -r "$HOME/.gemini" "$SANDBOX_CONFIG_DIR/.gemini"
        fi
    fi

    # Sync .ai-cli config (AI-CLI unified wrapper)
    mkdir -p "$SANDBOX_CONFIG_DIR/.ai-cli"
    if [ -f "$HOME/.ai-cli/config.json" ]; then
        if [ "$force" = true ] || [ ! -f "$SANDBOX_CONFIG_DIR/.ai-cli/config.json" ]; then
            echo "Syncing ~/.ai-cli/config.json ..."
            cp "$HOME/.ai-cli/config.json" "$SANDBOX_CONFIG_DIR/.ai-cli/"
        fi
    fi
    # .env from sandbox project directory (resolve symlinks)
    local src="$0"
    while [ -L "$src" ]; do
        local dir="$(cd "$(dirname "$src")" && pwd)"
        src="$(readlink "$src")"
        [[ $src != /* ]] && src="$dir/$src"
    done
    local script_path="$(cd "$(dirname "$src")" && pwd)"
    if [ -f "$script_path/.ai-cli.env" ]; then
        if [ "$force" = true ] || [ ! -f "$SANDBOX_CONFIG_DIR/.ai-cli/.env" ]; then
            echo "Syncing .ai-cli.env ..."
            cp "$script_path/.ai-cli.env" "$SANDBOX_CONFIG_DIR/.ai-cli/.env"
        fi
    fi

    # Copy fresh credentials from Keychain (always sync credentials)
    if command -v security >/dev/null 2>&1; then
        CREDS=$(security find-generic-password -s "Claude Code-credentials" -w 2>/dev/null)
        if [ -n "$CREDS" ]; then
            mkdir -p "$SANDBOX_CONFIG_DIR/.claude"
            echo "$CREDS" > "$SANDBOX_CONFIG_DIR/.claude/.credentials.json"
            chmod 600 "$SANDBOX_CONFIG_DIR/.claude/.credentials.json"
            echo "Synced credentials from Keychain"
        fi
    fi

    # Mark sandbox as initialized
    touch "$SANDBOX_CONFIG_DIR/.initialized"
    echo "‚úÖ Sandbox config synced to $SANDBOX_CONFIG_DIR"
}

# Determine which Claude config subfolder to use based on mode
if [[ "$GLM_MODE" == "true" ]]; then
    CLAUDE_CONFIG_SUBDIR=".claude-glm"
else
    CLAUDE_CONFIG_SUBDIR=".claude"
fi

# Function to setup GLM config on first GLM run
setup_glm_config() {
    local glm_dir="$SANDBOX_CONFIG_DIR/.claude-glm"

    if [[ ! -d "$glm_dir" ]]; then
        echo "üîß Setting up GLM config (first run)..."

        # Ensure base .claude config exists
        if [[ ! -d "$SANDBOX_CONFIG_DIR/.claude" ]]; then
            echo "Error: No base Claude config found. Run without -g first to sync configs."
            exit 1
        fi

        # Copy from existing Claude config
        cp -r "$SANDBOX_CONFIG_DIR/.claude" "$glm_dir"

        # Remove local npm installation (container uses global install)
        rm -rf "$glm_dir/local"

        # Remove Anthropic-specific credentials (GLM uses different auth)
        # These would cause authentication failures with z.ai proxy
        rm -f "$glm_dir/.credentials.json"
        rm -f "$glm_dir/.credentials"
        rm -f "$glm_dir/statsig_cache.json"
        rm -f "$glm_dir/stats-cache.json"
        rm -f "$glm_dir/.statsig"
        rm -rf "$glm_dir/statsig"
        rm -rf "$glm_dir/telemetry"

        echo "  Cleared Anthropic credentials and telemetry (GLM uses ANTHROPIC_AUTH_TOKEN instead)"

        # Copy GLM-specific settings.json from repo template (always overwrite - Claude's settings are wrong for GLM)
        if [[ -f "$SCRIPT_DIR/configs/glm/settings.json" ]]; then
            cp "$SCRIPT_DIR/configs/glm/settings.json" "$glm_dir/settings.json"
        fi

        # Copy GLM statusline script
        if [[ -f "$SCRIPT_DIR/configs/glm/statusline-robbyrussell.sh" ]]; then
            cp "$SCRIPT_DIR/configs/glm/statusline-robbyrussell.sh" "$glm_dir/"
            chmod +x "$glm_dir/statusline-robbyrussell.sh"
        fi

        # Copy GLM-specific MCP config from repo template (always overwrite)
        if [[ -f "$SCRIPT_DIR/configs/glm/.mcp.json" ]]; then
            cp "$SCRIPT_DIR/configs/glm/.mcp.json" "$glm_dir/"
        fi

        # Copy GLM-specific .claude.json (only if not already exists - preserve user's keys)
        if [[ -f "$SCRIPT_DIR/configs/glm/.claude.json" ]] && [[ ! -f "$SANDBOX_CONFIG_DIR/.claude-glm.json" ]]; then
            cp "$SCRIPT_DIR/configs/glm/.claude.json" "$SANDBOX_CONFIG_DIR/.claude-glm.json"
            echo "  Created GLM-specific .claude.json"
        fi

        echo "‚úÖ GLM config created at $glm_dir"
        echo "‚ö†Ô∏è  Add your ANTHROPIC_AUTH_TOKEN to $glm_dir/settings.json"
        echo "‚ö†Ô∏è  Add your API keys to $SANDBOX_CONFIG_DIR/.claude-glm.json"
    fi
}

# Handle sync-config mode
if [[ "$SYNC_CONFIG" == true ]]; then
    sync_sandbox_config
    exit 0
fi

# Handle remove mode
if [[ "$REMOVE_IMAGE" == true ]]; then
    HASH_FILE=".claude-code-sandbox-hash"
    
    if [[ ! -f "$SANDBOX_FILE" ]]; then
        echo "No workspace image to remove (no .claude-code-sandbox file found)"
        exit 0
    fi
    
    IMAGE_TO_REMOVE=$(cat "$SANDBOX_FILE")
    echo "=== Removing Claude Code Sandbox Workspace ==="
    echo "Image to remove: $IMAGE_TO_REMOVE"
    echo ""
    
    # Check if image exists
    if [[ -n "$(docker images -q $IMAGE_TO_REMOVE 2>/dev/null)" ]]; then
        echo "Removing Docker image..."
        if docker rmi "$IMAGE_TO_REMOVE"; then
            echo "‚úÖ Successfully removed image: $IMAGE_TO_REMOVE"
        else
            echo "‚ùå Failed to remove image: $IMAGE_TO_REMOVE"
            echo "   The image might be in use by a running container"
            exit 1
        fi
    else
        echo "‚ö†Ô∏è  Image not found in Docker: $IMAGE_TO_REMOVE"
    fi
    
    # Remove state files
    echo ""
    echo "Removing state files..."
    rm -f "$SANDBOX_FILE"
    echo "‚úÖ Removed .claude-code-sandbox"
    
    if [[ -f "$HASH_FILE" ]]; then
        rm -f "$HASH_FILE"
        echo "‚úÖ Removed .claude-code-sandbox-hash"
    fi
    
    echo ""
    echo "‚úÖ Workspace cleanup complete!"
    exit 0
fi

# Check if tools are specified without --build
if [[ "$BUILD_IMAGE" != true && -n "$INSTALL_TOOLS" ]]; then
    echo "Error: Tools can only be installed with --build flag"
    echo "Usage: $0 --build --install='plugin@version,plugin@version'"
    echo "Examples:"
    echo "  $0 --build --install='python@3.12.8,golang@1.21.5'"
    echo "  $0 --build --install='terraform@1.5.7,maven@3.9.6,kubectl@1.28.0'"
    echo "This ensures tools are installed at build time, not runtime"
    exit 1
fi

# Check for .tool-versions file if --build is specified but no --install
if [[ "$BUILD_IMAGE" == true && -z "$INSTALL_TOOLS" && -f ".tool-versions" ]]; then
    echo "Found .tool-versions file, parsing for tool installation..."
    # Parse .tool-versions file and convert to comma-separated format
    TOOL_VERSIONS_CONTENT=""
    while IFS= read -r line || [[ -n "$line" ]]; do
        # Skip empty lines and comments
        if [[ -n "$line" && ! "$line" =~ ^[[:space:]]*# ]]; then
            # Extract plugin and version
            read -r plugin version <<< "$line"
            if [[ -n "$plugin" && -n "$version" ]]; then
                if [[ -n "$TOOL_VERSIONS_CONTENT" ]]; then
                    TOOL_VERSIONS_CONTENT="$TOOL_VERSIONS_CONTENT,$plugin@$version"
                else
                    TOOL_VERSIONS_CONTENT="$plugin@$version"
                fi
            fi
        fi
    done < ".tool-versions"
    
    if [[ -n "$TOOL_VERSIONS_CONTENT" ]]; then
        INSTALL_TOOLS="$TOOL_VERSIONS_CONTENT"
        echo "Installing tools from .tool-versions: $INSTALL_TOOLS"
    fi
fi

# Get the directory where this script is located (handles symlinks, portable)
SCRIPT_SOURCE="$0"
while [ -L "$SCRIPT_SOURCE" ]; do
    SCRIPT_DIR="$(cd "$(dirname "$SCRIPT_SOURCE")" && pwd)"
    SCRIPT_SOURCE="$(readlink "$SCRIPT_SOURCE")"
    [[ $SCRIPT_SOURCE != /* ]] && SCRIPT_SOURCE="$SCRIPT_DIR/$SCRIPT_SOURCE"
done
SCRIPT_DIR="$(cd "$(dirname "$SCRIPT_SOURCE")" && pwd)"

# Check if base image needs to be built/updated
BASE_IMAGE_EXISTS="$(docker images -q $BASE_IMAGE_NAME 2> /dev/null)"
if [[ -z "$BASE_IMAGE_EXISTS" ]] || [[ "$REBUILD_IMAGE" == true ]]; then
    if [[ "$REBUILD_IMAGE" == true ]]; then
        echo "Rebuilding shared base image without cache: $BASE_IMAGE_NAME"
        docker build --no-cache -f "$SCRIPT_DIR/Dockerfile" --target base -t $BASE_IMAGE_NAME "$SCRIPT_DIR"
    else
        echo "Building shared base image: $BASE_IMAGE_NAME"
        docker build -f "$SCRIPT_DIR/Dockerfile" --target base -t $BASE_IMAGE_NAME "$SCRIPT_DIR"
    fi
fi

# Generate content hash for incremental build detection
CONTENT_HASH=$(get_content_hash "$SCRIPT_DIR")
HASH_FILE=".claude-code-sandbox-hash"

# Check if rebuild is needed
NEEDS_REBUILD=false
if [[ "$BUILD_IMAGE" == true ]]; then
    NEEDS_REBUILD=true
    echo "Forced rebuild requested"
elif [[ -n "$INSTALL_TOOLS" ]]; then
    NEEDS_REBUILD=true
    echo "Tools specified, rebuild needed"
elif [[ "$(docker images -q $IMAGE_NAME 2> /dev/null)" == "" ]]; then
    NEEDS_REBUILD=true
    echo "Image doesn't exist, rebuild needed"
    # Auto-detect .tool-versions for new workspaces
    if [[ -f ".tool-versions" && -z "$INSTALL_TOOLS" ]]; then
        echo "Found .tool-versions file, parsing for tool installation..."
        # Parse .tool-versions file and convert to comma-separated format
        TOOL_VERSIONS_CONTENT=""
        while IFS= read -r line || [[ -n "$line" ]]; do
            # Skip empty lines and comments
            if [[ -n "$line" && ! "$line" =~ ^[[:space:]]*# ]]; then
                # Extract plugin and version
                read -r plugin version <<< "$line"
                if [[ -n "$plugin" && -n "$version" ]]; then
                    if [[ -n "$TOOL_VERSIONS_CONTENT" ]]; then
                        TOOL_VERSIONS_CONTENT="$TOOL_VERSIONS_CONTENT,$plugin@$version"
                    else
                        TOOL_VERSIONS_CONTENT="$plugin@$version"
                    fi
                fi
            fi
        done < ".tool-versions"
        
        if [[ -n "$TOOL_VERSIONS_CONTENT" ]]; then
            INSTALL_TOOLS="$TOOL_VERSIONS_CONTENT"
            echo "Installing tools from .tool-versions: $INSTALL_TOOLS"
        fi
    fi
elif [[ ! -f "$HASH_FILE" ]] || [[ "$(cat "$HASH_FILE" 2>/dev/null)" != "$CONTENT_HASH" ]]; then
    NEEDS_REBUILD=true
    echo "Configuration changed, incremental rebuild needed"
fi

# Build workspace-specific image if needed
if [[ "$NEEDS_REBUILD" == true ]]; then
    echo "Building workspace image: $IMAGE_NAME"
    BUILD_ARGS="--build-arg BASE_IMAGE=$BASE_IMAGE_NAME"
    if [ -n "$INSTALL_TOOLS" ]; then
        BUILD_ARGS="$BUILD_ARGS --build-arg INSTALL_TOOLS='$INSTALL_TOOLS'"
        echo "Installing tools: $INSTALL_TOOLS"
    fi
    
    # Add --no-cache flag if rebuild is requested
    if [[ "$REBUILD_IMAGE" == true ]]; then
        echo "Forcing full rebuild without cache..."
        BUILD_ARGS="--no-cache $BUILD_ARGS"
    fi
    
    docker build $BUILD_ARGS -f "$SCRIPT_DIR/Dockerfile" --target workspace -t $IMAGE_NAME "$SCRIPT_DIR"
    
    # Save content hash for future incremental builds
    echo "$CONTENT_HASH" > "$HASH_FILE"
else
    echo "No changes detected, using existing image: $IMAGE_NAME"
fi


# Check for Claude configuration directories
CLAUDE_CONFIG_ARGS=()

# Auto-sync sandbox config on first run only (check for .initialized marker)
if [ ! -f "$SANDBOX_CONFIG_DIR/.initialized" ]; then
    echo "First run detected - syncing configs to sandbox..."
    sync_sandbox_config false  # false = don't overwrite existing configs
fi

# Setup GLM config on first GLM run
if [[ "$GLM_MODE" == "true" ]]; then
    setup_glm_config
fi

# Mount sandbox configs (persistent, protects host files)
# Use CLAUDE_CONFIG_SUBDIR to support both Claude and GLM modes
if [ -d "$SANDBOX_CONFIG_DIR/$CLAUDE_CONFIG_SUBDIR" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/$CLAUDE_CONFIG_SUBDIR:/home/node/.claude")
    echo "Using Claude config: $SANDBOX_CONFIG_DIR/$CLAUDE_CONFIG_SUBDIR"
fi

if [ -d "$SANDBOX_CONFIG_DIR/.config/claude" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/.config/claude:/home/node/.config/claude")
    echo "Using sandbox config: .config/claude"
fi

if [ -d "$SANDBOX_CONFIG_DIR/.anthropic" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/.anthropic:/home/node/.anthropic")
    echo "Using sandbox config: .anthropic"
fi

# Linux-specific cache directory (isolates platform-specific compiled artifacts)
LINUX_CACHE_DIR="$HOME/.claude-sandbox-cache"
mkdir -p "$LINUX_CACHE_DIR" || { echo "Error: Failed to create cache directory $LINUX_CACHE_DIR" >&2; exit 1; }

# Helper: mount a Linux-isolated cache directory
add_linux_cache() {
    local name="$1"
    mkdir -p "$LINUX_CACHE_DIR/$name" || { echo "Error: Failed to create cache directory $LINUX_CACHE_DIR/$name" >&2; exit 1; }
    CLAUDE_CONFIG_ARGS+=(-v "$LINUX_CACHE_DIR/$name:/home/node/$name")
    echo "Using Linux cache: $name"
}

# Mount Linux-specific caches (auto-created, isolated from macOS host caches)
# These contain platform-specific compiled artifacts that don't cross-compile well
add_linux_cache .cargo      # Rust compiled deps
add_linux_cache go          # Go binaries
add_linux_cache .bun        # Bun native binaries
add_linux_cache .cache      # pip wheels, yarn, pnpm, go-build, deno, composer
add_linux_cache .pnpm-store # pnpm native deps
add_linux_cache .gem        # Ruby native extensions
add_linux_cache .bundle     # Ruby bundler native deps
add_linux_cache .m2         # Maven JARs
add_linux_cache .gradle     # Gradle cache
add_linux_cache .npm        # npm cache
add_linux_cache .nuget      # NuGet packages

# Mount sandbox .claude.json (use GLM-specific version in GLM mode)
if [[ "$GLM_MODE" == "true" ]] && [ -f "$SANDBOX_CONFIG_DIR/.claude-glm.json" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/.claude-glm.json:/home/node/.claude.json")
    echo "Using GLM config: .claude-glm.json"
elif [ -f "$SANDBOX_CONFIG_DIR/.claude.json" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/.claude.json:/home/node/.claude.json")
    echo "Using sandbox config: .claude.json"
fi

# Mount sandbox opencode config
if [ -d "$SANDBOX_CONFIG_DIR/.config/opencode" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/.config/opencode:/home/node/.config/opencode")
    echo "Using sandbox config: .config/opencode"
fi

# Mount sandbox opencode data
if [ -d "$SANDBOX_CONFIG_DIR/.local/share/opencode" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/.local/share/opencode:/home/node/.local/share/opencode")
    echo "Using sandbox config: .local/share/opencode"
fi

# Mount sandbox Qwen Code config
if [ -d "$SANDBOX_CONFIG_DIR/.qwen" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/.qwen:/home/node/.qwen")
    echo "Using sandbox config: .qwen"
fi

# Mount sandbox Codex config (OpenAI Codex CLI)
if [ -d "$SANDBOX_CONFIG_DIR/.codex" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/.codex:/home/node/.codex")
    echo "Using sandbox config: .codex"
fi

# Mount sandbox Gemini config (Google Gemini CLI)
if [ -d "$SANDBOX_CONFIG_DIR/.gemini" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/.gemini:/home/node/.gemini")
    echo "Using sandbox config: .gemini"
fi

# Mount AI-CLI config files (not the whole dir - repo is cloned there)
if [ -f "$SANDBOX_CONFIG_DIR/.ai-cli/config.json" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/.ai-cli/config.json:/home/node/.ai-cli/config.json")
    echo "Using sandbox config: .ai-cli/config.json"
fi
if [ -f "$SANDBOX_CONFIG_DIR/.ai-cli/.env" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/.ai-cli/.env:/home/node/.ai-cli/.env")
    echo "Using sandbox config: .ai-cli/.env"
fi

# Mount sandbox opencode cache (overrides host ~/.cache/opencode)
if [ -d "$SANDBOX_CONFIG_DIR/.cache/opencode" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/.cache/opencode:/home/node/.cache/opencode")
    echo "Using sandbox config: .cache/opencode"
fi

# Mount sandbox opencode state (model selection, favorites, history)
if [ -d "$SANDBOX_CONFIG_DIR/.local/state/opencode" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/.local/state/opencode:/home/node/.local/state/opencode")
    echo "Using sandbox config: .local/state/opencode"
fi

# Check for basic-memory directory (basic-memory MCP server)
if [ -d "$HOME/basic-memory" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$HOME/basic-memory:/home/node/basic-memory")
    echo "Mounting basic-memory MCP server from $HOME/basic-memory"
fi

# Check for .beads directory (beads task management)
# Use array to properly handle paths with spaces
BEADS_MOUNT_ARGS=()
if [ -d "$(pwd)/.beads" ]; then
    BEADS_MOUNT_ARGS=(-v "$(pwd)/.beads:/workspace/.beads")
    echo "Mounting beads task directory from $(pwd)/.beads"
fi

# Add Docker socket mounting if Docker flag is enabled
DOCKER_MOUNT=""
if [ "$DOCKER_ENABLED" = true ]; then
    if [ -S "/var/run/docker.sock" ]; then
        DOCKER_MOUNT="-v /var/run/docker.sock:/var/run/docker.sock"
        echo "Mounting Docker socket for container access"
    else
        echo "Warning: Docker socket not found at /var/run/docker.sock"
        echo "Docker commands may not work inside the container"
    fi
fi

if [ ${#CLAUDE_CONFIG_ARGS[@]} -eq 0 ]; then
    echo "Warning: No configuration directories found"
fi

# Check if we have a TTY or if non-interactive mode is requested
if [ -t 0 ] || [ "$NON_INTERACTIVE" = true ]; then
    if [ "$SHELL_MODE" = true ]; then
        # Run interactive shell instead of Claude Code
        docker run -it --rm \
            --user node \
            -v "$(pwd):/workspace" \
            "${BEADS_MOUNT_ARGS[@]}" \
            "${CLAUDE_CONFIG_ARGS[@]}" \
            $DOCKER_MOUNT \
            -e TERM="$TERM" \
            --init \
            "$IMAGE_NAME" \
            bash -c "source ~/.asdf/asdf.sh 2>/dev/null || true; if [ ! -d /workspace/.venv-linux ] && { [ -f /workspace/requirements.txt ] || [ -f /workspace/pyproject.toml ]; }; then echo 'Creating Python venv (.venv-linux)...'; python3 -m venv /workspace/.venv-linux; if [ -f /workspace/requirements.txt ]; then /workspace/.venv-linux/bin/pip install -q -r /workspace/requirements.txt; elif [ -f /workspace/pyproject.toml ]; then /workspace/.venv-linux/bin/pip install -q -e /workspace; fi; fi; if [ -f /workspace/.venv-linux/bin/activate ]; then source /workspace/.venv-linux/bin/activate; fi; exec bash"
    elif [ "$NON_INTERACTIVE" = true ]; then
        # Run non-interactive command with asdf support
        docker run -it --rm \
            --user node \
            -v "$(pwd):/workspace" \
            "${BEADS_MOUNT_ARGS[@]}" \
            "${CLAUDE_CONFIG_ARGS[@]}" \
            $DOCKER_MOUNT \
            -e TERM="$TERM" \
            --init \
            "$IMAGE_NAME" \
            bash -c "export PATH=\"\$HOME/.npm-global/bin:\$HOME/.local/bin:\$PATH\"; source ~/.asdf/asdf.sh 2>/dev/null || true; if [ ! -d /workspace/.venv-linux ] && { [ -f /workspace/requirements.txt ] || [ -f /workspace/pyproject.toml ]; }; then echo 'Creating Python venv (.venv-linux)...'; python3 -m venv /workspace/.venv-linux; if [ -f /workspace/requirements.txt ]; then /workspace/.venv-linux/bin/pip install -q -r /workspace/requirements.txt; elif [ -f /workspace/pyproject.toml ]; then /workspace/.venv-linux/bin/pip install -q -e /workspace; fi; fi; if [ -f /workspace/.venv-linux/bin/activate ]; then source /workspace/.venv-linux/bin/activate; fi; $*"
    else
        # Run the container with current directory mounted as workspace
        docker run -it --rm \
            --user node \
            -v "$(pwd):/workspace" \
            "${BEADS_MOUNT_ARGS[@]}" \
            "${CLAUDE_CONFIG_ARGS[@]}" \
            $DOCKER_MOUNT \
            -e TERM="$TERM" \
            --init \
            "$IMAGE_NAME" \
            bash -c "export PATH=\"\$HOME/.npm-global/bin:\$HOME/.local/bin:\$PATH\"; source ~/.asdf/asdf.sh 2>/dev/null || true; if [ ! -d /workspace/.venv-linux ] && { [ -f /workspace/requirements.txt ] || [ -f /workspace/pyproject.toml ]; }; then echo 'Creating Python venv (.venv-linux)...'; python3 -m venv /workspace/.venv-linux; if [ -f /workspace/requirements.txt ]; then /workspace/.venv-linux/bin/pip install -q -r /workspace/requirements.txt; elif [ -f /workspace/pyproject.toml ]; then /workspace/.venv-linux/bin/pip install -q -e /workspace; fi; fi; if [ -f /workspace/.venv-linux/bin/activate ]; then source /workspace/.venv-linux/bin/activate; fi; if ! command -v node >/dev/null 2>&1; then echo 'No Node.js found, installing LTS...'; asdf plugin add nodejs 2>/dev/null || true; asdf install nodejs lts; asdf global nodejs lts; fi; claude-wrapper --dangerously-skip-permissions $*"
    fi
else
    echo "Error: This script requires an interactive terminal (TTY)."
    echo "Please run this script from a regular terminal, not from within another program."
    echo "The script is designed to launch an interactive Claude session."
    echo "Use -n or --non-interactive to run commands without TTY."
    exit 1
fi