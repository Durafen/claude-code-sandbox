#!/bin/bash
set -e

# Enable BuildKit for faster parallel builds and better caching
export DOCKER_BUILDKIT=1

# Error handler - show big error and exit
trap 'echo ""; echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"; echo "  ‚ùå ERROR: Command failed at line $LINENO"; echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"; echo ""; exit 1' ERR

# Sandboxed Claude Code CLI wrapper
# Based on https://github.com/cwensel/claude-sandbox/blob/main/claudes

# Function to generate random suffix
generate_random_suffix() {
    python3 -c "import random, string; print(''.join(random.choices(string.ascii_lowercase + string.digits, k=8)))"
}

# Define base image name (shared across all workspaces)
BASE_IMAGE_NAME="claude-code-sandbox-base"

# Function to get content hash for change detection
get_content_hash() {
    {
        # Hash Dockerfile content
        cat "$1/Dockerfile" 2>/dev/null || echo "no-dockerfile"
        # Hash .tool-versions if present
        cat ".tool-versions" 2>/dev/null || echo "no-tool-versions" 
    } | sha256sum | cut -d' ' -f1 | head -c 8
}

# Determine image name based on local .claude-code-sandbox file or create new one
SANDBOX_FILE=".claude-code-sandbox"
if [ -f "$SANDBOX_FILE" ]; then
    IMAGE_NAME=$(cat "$SANDBOX_FILE")
    echo "Using existing workspace image: $IMAGE_NAME"
else
    # Generate new image name with random suffix
    RANDOM_SUFFIX=$(generate_random_suffix)
    IMAGE_NAME="claude-code-sandbox-${RANDOM_SUFFIX}"
    echo "$IMAGE_NAME" > "$SANDBOX_FILE"
    echo "Created new workspace image: $IMAGE_NAME"
fi

# Helper: Generate venv setup bash snippet (DRY - used by all docker run commands)
# Uses $CONTAINER_WORKSPACE env var passed to container for proper quoting
generate_venv_setup() {
    cat <<'VENV_EOF'
if [ ! -d "$CONTAINER_WORKSPACE/.venv-linux" ] && { [ -f "$CONTAINER_WORKSPACE/requirements.txt" ] || [ -f "$CONTAINER_WORKSPACE/pyproject.toml" ]; }; then
    echo 'Creating Python venv (.venv-linux)...'
    if python3 -m venv "$CONTAINER_WORKSPACE/.venv-linux"; then
        "$CONTAINER_WORKSPACE/.venv-linux/bin/pip" install -q --upgrade pip 2>/dev/null
        if [ -f "$CONTAINER_WORKSPACE/requirements.txt" ]; then
            if ! "$CONTAINER_WORKSPACE/.venv-linux/bin/pip" install -q -r "$CONTAINER_WORKSPACE/requirements.txt"; then
                echo 'Warning: Some packages failed to install from requirements.txt'
            fi
        elif [ -f "$CONTAINER_WORKSPACE/pyproject.toml" ]; then
            if ! "$CONTAINER_WORKSPACE/.venv-linux/bin/pip" install -q -e "$CONTAINER_WORKSPACE"; then
                echo 'Warning: Failed to install package from pyproject.toml'
            fi
        fi
    else
        echo 'Warning: Failed to create Python venv'
    fi
fi
if [ -f "$CONTAINER_WORKSPACE/.venv-linux/bin/activate" ]; then
    source "$CONTAINER_WORKSPACE/.venv-linux/bin/activate"
fi
VENV_EOF
}

# Parse command line options
BUILD_IMAGE=false
SHELL_MODE=false
NON_INTERACTIVE=false
DOCKER_ENABLED=false
INSTALL_TOOLS=""
CLEANUP_MODE=false
CLEANUP_DAYS=7
DRY_RUN=false
REBUILD_IMAGE=false
REMOVE_IMAGE=false
SYNC_CONFIG=false
CLAUDE_MODE=true
GLM_MODE=false
WORKTREE_BRANCH=""
CLEANUP_WORKTREE=false

while getopts "bsndcgw:W-:" opt; do
    case $opt in
        b)
            BUILD_IMAGE=true
            ;;
        s)
            SHELL_MODE=true
            ;;
        n)
            NON_INTERACTIVE=true
            ;;
        d)
            DOCKER_ENABLED=true
            ;;
        c)
            CLAUDE_MODE=true
            SHELL_MODE=false
            ;;
        g)
            GLM_MODE=true
            CLAUDE_MODE=true
            SHELL_MODE=false
            ;;
        w)
            WORKTREE_BRANCH="$OPTARG"
            if [ -z "$WORKTREE_BRANCH" ]; then
                echo "Error: -w requires a branch name" >&2
                exit 1
            fi
            ;;
        W)
            CLEANUP_WORKTREE=true
            ;;
        -)
            case "${OPTARG}" in
                build)
                    BUILD_IMAGE=true
                    ;;
                install=*)
                    INSTALL_TOOLS="${OPTARG#*=}"
                    ;;
                shell)
                    SHELL_MODE=true
                    ;;
                non-interactive)
                    NON_INTERACTIVE=true
                    ;;
                docker)
                    DOCKER_ENABLED=true
                    ;;
                claude)
                    CLAUDE_MODE=true
                    SHELL_MODE=false
                    ;;
                glm)
                    GLM_MODE=true
                    CLAUDE_MODE=true
                    SHELL_MODE=false
                    ;;
                cleanup)
                    CLEANUP_MODE=true
                    ;;
                sync-config)
                    SYNC_CONFIG=true
                    ;;
                older-than=*)
                    CLEANUP_DAYS="${OPTARG#*=}"
                    # Parse days from format like "7d", "3", "14d"
                    CLEANUP_DAYS="${CLEANUP_DAYS%d}"
                    ;;
                dry-run)
                    DRY_RUN=true
                    ;;
                rebuild)
                    REBUILD_IMAGE=true
                    BUILD_IMAGE=true
                    ;;
                remove)
                    REMOVE_IMAGE=true
                    ;;
                worktree-branch=*)
                    WORKTREE_BRANCH="${OPTARG#*=}"
                    if [ -z "$WORKTREE_BRANCH" ]; then
                        echo "Error: --worktree-branch requires a branch name" >&2
                        exit 1
                    fi
                    ;;
                cleanup-worktree)
                    CLEANUP_WORKTREE=true
                    ;;
                help)
                    echo "Claude Code Sandbox - Containerized development environment"
                    echo ""
                    echo "Usage: $0 [OPTIONS] [CLAUDE_ARGS...]"
                    echo ""
                    echo "Options:"
                    echo "  -b, --build                   Build/rebuild the Docker image"
                    echo "  -c, --claude                  Launch Claude Code (default)"
                    echo "  -g, --glm                     Launch Claude Code in GLM mode (z.ai proxy)"
                    echo "  -s, --shell                   Launch interactive shell"
                    echo "  -n, --non-interactive CMD     Run non-interactive command"
                    echo "  -d, --docker                  Enable Docker access inside container"
                    echo "      --install=TOOLS           Install tools at build time"
                    echo "      --rebuild                 Force full rebuild without cache"
                    echo "      --remove                  Remove workspace image and state files"
                    echo "      --cleanup                 Remove old workspace images (default: 7+ days)"
                    echo "  -W, --cleanup-worktree        Remove worktree after session ends"
                    echo "  -w, --worktree-branch=BRANCH  Create/use a git worktree for the specified branch"
                    echo "      --sync-config             Sync host configs to sandbox (overwrites sandbox configs)"

                    echo "      --older-than=DAYS         Cleanup threshold (e.g. --older-than=3d)"
                    echo "      --dry-run                 Show what would be cleaned without doing it"
                    echo "      --help                    Show this help message"
                    echo ""
                    echo "Worktree Support:"
                    echo "  -w BRANCH, --worktree-branch=BRANCH"
                    echo "      Creates or uses a git worktree for the specified branch."
                    echo "      - Auto-generates path: ../<current-dir>-<branch> (parent directory)"
                    echo "        with slashes converted to dashes (e.g., in 'crm': feat/test -> ../crm-feat-test)."
                    echo "      - Supports both absolute and relative paths."
                    echo "      - Auto-creates branch from HEAD if it doesn't exist."
                    echo "      - Copies .env* and *.db files to new worktrees automatically."
                    echo "      - Works with all modes: shell (-s), claude (-c), non-interactive (-n)."
                    echo ""
                    echo "  -W, --cleanup-worktree"
                    echo "      Removes the worktree after the script completes execution."
                    echo ""
                    echo "Tool Installation:"
                    echo "  --install='tool1@version1,tool2@version2'"
                    echo ""
                    echo "Examples:"
                    echo "  $0 --build --install='python@3.12.8,golang@1.21.5'"
                    echo "  $0 --build --install='java@adoptopenjdk-17.0.2+8,terraform@1.5.7'"
                    echo "  $0 --rebuild                  # Force complete rebuild without cache"
                    echo "  $0 --remove                   # Remove current workspace image"
                    echo "  $0 --shell                    # Interactive shell"
                    echo "  $0 -n 'python --version'     # Non-interactive command"
                    echo "  $0 python --version          # Same as -n (positional args)"
                    echo "  $0 --cleanup                  # Remove images older than 7 days"
                    echo "  $0 --cleanup --older-than=3d  # Remove images older than 3 days"
                    echo "  $0 --cleanup --dry-run        # Preview cleanup without removing"
                    echo "  $0 -w feat/test              # Create worktree at ../crm-feat-test (if in 'crm' dir)"
                    echo "  $0 -w ../crm2                 # Use existing relative path as worktree"
                    echo "  $0 -w feat/test -W            # Create worktree, cleanup on exit"
                    echo ""
                    echo "Auto-completion:"
                    echo "  Bash: source ./completions/claude-code-sandbox"
                    echo "  Zsh:  Add ./completions to your fpath and run: autoload -U compinit && compinit"
                    echo ""
                    echo "For more information, see: README.md"
                    exit 0
                    ;;
                *)
                    echo "Unknown option --${OPTARG}" >&2
                    echo "Usage: $0 --build --install='plugin1@version1,plugin2@version2'"
                    echo "Examples:"
                    echo "  $0 --build --install='python@3.12.8,golang@1.21.5,maven@3.9.6'"
                    echo "  $0 --build --install='java@adoptopenjdk-17.0.2+8,terraform@1.5.7'"
                    echo "  $0 --build --install='nodejs@20.11.0,rust@1.75.0'"
                    echo "  $0 -w feat/test"
                    echo "  $0 -w ../crm2"
                    echo "  $0 -w feat/test -W"
                    echo "Use --help for more information"
                    exit 1
                    ;;
            esac
            ;;
        \?)
            echo "Invalid option: -$OPTARG" >&2
            exit 1
            ;;
    esac
done

shift $((OPTIND-1))

# If positional arguments remain, treat them as a non-interactive command
if [[ $# -gt 0 ]]; then
    NON_INTERACTIVE=true
    SHELL_MODE=false
    set -- "$*"  # Combine all args into single command string
fi

# Function to perform image cleanup
cleanup_images() {
    local days=$1
    local dry_run=$2
    local current_image=$(cat "$SANDBOX_FILE" 2>/dev/null || echo "")
    
    echo "=== Claude Code Sandbox Image Cleanup ==="
    echo "Looking for workspace images older than $days days..."
    echo "Current workspace image: $current_image"
    echo "Base image (always preserved): $BASE_IMAGE_NAME"
    echo ""
    
    # Get current timestamp in seconds
    local current_time=$(date +%s)
    local cutoff_time=$((current_time - (days * 24 * 3600)))
    
    # Find all claude-code-sandbox workspace images (exclude base)
    local images_to_check=$(docker images --format "{{.Repository}}\t{{.Tag}}\t{{.CreatedAt}}\t{{.Size}}\t{{.ID}}" | \
        grep "^claude-code-sandbox-" | grep -v "^$BASE_IMAGE_NAME")
    
    if [[ -z "$images_to_check" ]]; then
        echo "No workspace images found to clean up."
        return 0
    fi
    
    local images_to_remove=()
    local total_size=0
    
    echo "Scanning workspace images:"
    echo "Repository                        Created                Size      Image ID       Status"
    echo "----------------------------------------------------------------------------------------"
    
    # Process each image
    while IFS=$'\t' read -r repo tag created_at size image_id; do
        if [[ -z "$repo" ]]; then continue; fi
        
        # Use repo name directly (it should be the image name)
        local image_name="$repo"
        
        # Skip if this is the current workspace image
        if [[ "$image_name" == "$current_image" ]]; then
            printf "%-32s %-22s %-9s %-14s %s\n" "$repo" "$created_at" "$size" "$image_id" "(CURRENT - PRESERVED)"
            continue
        fi
        
        # Parse creation time - remove timezone for compatibility
        local clean_date=$(echo "$created_at" | sed 's/ -[0-9]* [A-Z]*$//')
        local image_time
        if command -v gdate >/dev/null 2>&1; then
            # macOS with coreutils
            image_time=$(gdate -d "$clean_date" +%s 2>/dev/null || echo 0)
        else
            # Linux date
            image_time=$(date -d "$clean_date" +%s 2>/dev/null || echo 0)
        fi
        
        if [[ $image_time -lt $cutoff_time ]]; then
            printf "%-32s %-22s %-9s %-14s %s\n" "$repo" "$created_at" "$size" "$image_id" "(MARKED FOR REMOVAL)"
            images_to_remove+=("$image_name")
            # Convert size to bytes for summing (rough estimate)
            local size_mb=$(echo "$size" | sed 's/[^0-9.]//g')
            if [[ -n "$size_mb" && "$size_mb" != "" ]]; then
                total_size=$((total_size + ${size_mb%.*}))
            fi
        else
            printf "%-32s %-22s %-9s %-14s %s\n" "$repo" "$created_at" "$size" "$image_id" "(TOO RECENT - PRESERVED)"
        fi
    done <<< "$images_to_check"
    
    echo ""
    
    if [[ ${#images_to_remove[@]} -eq 0 ]]; then
        echo "‚úÖ No images need cleanup (all are either current workspace, base image, or too recent)"
        return 0
    fi
    
    echo "üìã Summary:"
    echo "  Images to remove: ${#images_to_remove[@]}"
    echo "  Estimated space to reclaim: ~${total_size}MB"
    echo ""
    
    if [[ "$dry_run" == "true" ]]; then
        echo "üîç DRY RUN - No images will actually be removed"
        echo "Images that would be removed:"
        for image in "${images_to_remove[@]}"; do
            echo "  - $image"
        done
        return 0
    fi
    
    # Confirm removal
    echo "‚ö†Ô∏è  This will permanently remove ${#images_to_remove[@]} Docker images."
    echo "Are you sure you want to continue? (y/N)"
    read -r confirmation
    
    if [[ ! "$confirmation" =~ ^[Yy]$ ]]; then
        echo "‚ùå Cleanup cancelled"
        return 0
    fi
    
    echo ""
    echo "üóëÔ∏è  Removing images..."
    
    local removed_count=0
    for image in "${images_to_remove[@]}"; do
        echo "Removing: $image"
        if docker rmi "$image" >/dev/null 2>&1; then
            echo "  ‚úÖ Removed successfully"
            ((removed_count++))
        else
            echo "  ‚ö†Ô∏è  Failed to remove (might be in use)"
        fi
    done
    
    echo ""
    echo "‚úÖ Cleanup complete!"
    echo "  Successfully removed: $removed_count/${#images_to_remove[@]} images"
    
    if [[ $removed_count -gt 0 ]]; then
        echo ""
        echo "üí° Consider running 'docker system prune' to reclaim additional space from dangling images and build cache"
    fi
}

# Handle cleanup mode
if [[ "$CLEANUP_MODE" == true ]]; then
    cleanup_images "$CLEANUP_DAYS" "$DRY_RUN"
    exit 0
fi

# Sandbox config directory (persistent, protects host files)
SANDBOX_CONFIG_DIR="$HOME/.claude-sandbox"

# Function to sync credentials from Keychain
# Compares content with cached file, writes only if different
sync_credentials() {
    # Skip if not on macOS or no security command
    if ! command -v security >/dev/null 2>&1; then
        return 0
    fi

    mkdir -p "$SANDBOX_CONFIG_DIR/.claude"
    local creds_file="$SANDBOX_CONFIG_DIR/.claude/.credentials.json"

    # Fetch credentials from Keychain
    local creds=$(security find-generic-password -s "Claude Code-credentials" -w 2>/dev/null)
    if [ -z "$creds" ]; then
        return 0
    fi

    # Compare with cached - write only if different
    if [ -f "$creds_file" ]; then
        local cached=$(cat "$creds_file" 2>/dev/null)
        if [ "$creds" = "$cached" ]; then
            return 0
        fi
    fi

    echo "$creds" > "$creds_file"
    chmod 600 "$creds_file"
    echo "Synced credentials from Keychain"
}

# Function to sync host configs to sandbox (force mode overwrites existing)
sync_sandbox_config() {
    local force=${1:-true}  # Default to force mode for --sync-config
    echo "=== Syncing host configs to sandbox ==="
    mkdir -p "$SANDBOX_CONFIG_DIR"

    # Sync .claude directory (skip broken symlinks and local npm install)
    if [ -d "$HOME/.claude" ]; then
        if [ "$force" = true ] || [ ! -d "$SANDBOX_CONFIG_DIR/.claude" ]; then
            echo "Syncing ~/.claude ..."
            rm -rf "$SANDBOX_CONFIG_DIR/.claude"
            mkdir -p "$SANDBOX_CONFIG_DIR/.claude"
            rsync -a --ignore-errors --exclude='local' "$HOME/.claude/" "$SANDBOX_CONFIG_DIR/.claude/" 2>/dev/null
        fi
    fi

    # Sync .claude.json
    if [ -f "$HOME/.claude.json" ]; then
        if [ "$force" = true ] || [ ! -f "$SANDBOX_CONFIG_DIR/.claude.json" ]; then
            echo "Syncing ~/.claude.json ..."
            cp "$HOME/.claude.json" "$SANDBOX_CONFIG_DIR/.claude.json"
        fi
    fi

    # Sync .config/claude
    if [ -d "$HOME/.config/claude" ]; then
        if [ "$force" = true ] || [ ! -d "$SANDBOX_CONFIG_DIR/.config/claude" ]; then
            echo "Syncing ~/.config/claude ..."
            mkdir -p "$SANDBOX_CONFIG_DIR/.config"
            rm -rf "$SANDBOX_CONFIG_DIR/.config/claude"
            cp -r "$HOME/.config/claude" "$SANDBOX_CONFIG_DIR/.config/claude"
        fi
    fi

    # Sync .anthropic
    if [ -d "$HOME/.anthropic" ]; then
        if [ "$force" = true ] || [ ! -d "$SANDBOX_CONFIG_DIR/.anthropic" ]; then
            echo "Syncing ~/.anthropic ..."
            rm -rf "$SANDBOX_CONFIG_DIR/.anthropic"
            cp -r "$HOME/.anthropic" "$SANDBOX_CONFIG_DIR/.anthropic"
        fi
    fi

    # Sync .config/opencode
    if [ -d "$HOME/.config/opencode" ]; then
        if [ "$force" = true ] || [ ! -d "$SANDBOX_CONFIG_DIR/.config/opencode" ]; then
            echo "Syncing ~/.config/opencode ..."
            mkdir -p "$SANDBOX_CONFIG_DIR/.config"
            rm -rf "$SANDBOX_CONFIG_DIR/.config/opencode"
            cp -r "$HOME/.config/opencode" "$SANDBOX_CONFIG_DIR/.config/opencode"
        fi
    fi

    # Sync .local/share/opencode
    if [ -d "$HOME/.local/share/opencode" ]; then
        if [ "$force" = true ] || [ ! -d "$SANDBOX_CONFIG_DIR/.local/share/opencode" ]; then
            echo "Syncing ~/.local/share/opencode ..."
            mkdir -p "$SANDBOX_CONFIG_DIR/.local/share"
            rm -rf "$SANDBOX_CONFIG_DIR/.local/share/opencode"
            cp -r "$HOME/.local/share/opencode" "$SANDBOX_CONFIG_DIR/.local/share/opencode"
        fi
    fi

    # Sync .cache/opencode (isolated from host cache)
    if [ -d "$HOME/.cache/opencode" ]; then
        if [ "$force" = true ] || [ ! -d "$SANDBOX_CONFIG_DIR/.cache/opencode" ]; then
            echo "Syncing ~/.cache/opencode ..."
            mkdir -p "$SANDBOX_CONFIG_DIR/.cache"
            rm -rf "$SANDBOX_CONFIG_DIR/.cache/opencode"
            cp -r "$HOME/.cache/opencode" "$SANDBOX_CONFIG_DIR/.cache/opencode"
        fi
    fi

    # Sync .local/state/opencode (model selection, favorites, history)
    if [ -d "$HOME/.local/state/opencode" ]; then
        if [ "$force" = true ] || [ ! -d "$SANDBOX_CONFIG_DIR/.local/state/opencode" ]; then
            echo "Syncing ~/.local/state/opencode ..."
            mkdir -p "$SANDBOX_CONFIG_DIR/.local/state"
            rm -rf "$SANDBOX_CONFIG_DIR/.local/state/opencode"
            cp -r "$HOME/.local/state/opencode" "$SANDBOX_CONFIG_DIR/.local/state/opencode"
        fi
    fi

    # Sync .qwen (Qwen Code) - always sync newer files (OAuth tokens refresh)
    if [ -d "$HOME/.qwen" ]; then
        if [ "$force" = true ] || [ ! -d "$SANDBOX_CONFIG_DIR/.qwen" ]; then
            echo "Syncing ~/.qwen ..."
            rm -rf "$SANDBOX_CONFIG_DIR/.qwen"
            cp -r "$HOME/.qwen" "$SANDBOX_CONFIG_DIR/.qwen"
        else
            rsync -a --update "$HOME/.qwen/" "$SANDBOX_CONFIG_DIR/.qwen/"
        fi
    fi

    # Sync .codex (OpenAI Codex CLI)
    if [ -d "$HOME/.codex" ]; then
        if [ "$force" = true ] || [ ! -d "$SANDBOX_CONFIG_DIR/.codex" ]; then
            echo "Syncing ~/.codex ..."
            rm -rf "$SANDBOX_CONFIG_DIR/.codex"
            cp -r "$HOME/.codex" "$SANDBOX_CONFIG_DIR/.codex"
        fi
    fi

    # Sync .gemini (Google Gemini CLI)
    if [ -d "$HOME/.gemini" ]; then
        if [ "$force" = true ] || [ ! -d "$SANDBOX_CONFIG_DIR/.gemini" ]; then
            echo "Syncing ~/.gemini ..."
            rm -rf "$SANDBOX_CONFIG_DIR/.gemini"
            cp -r "$HOME/.gemini" "$SANDBOX_CONFIG_DIR/.gemini"
        fi
    fi

    # Sync .ai-cli config (AI-CLI unified wrapper)
    mkdir -p "$SANDBOX_CONFIG_DIR/.ai-cli"
    if [ -f "$HOME/.ai-cli/config.json" ]; then
        if [ "$force" = true ] || [ ! -f "$SANDBOX_CONFIG_DIR/.ai-cli/config.json" ]; then
            echo "Syncing ~/.ai-cli/config.json ..."
            cp "$HOME/.ai-cli/config.json" "$SANDBOX_CONFIG_DIR/.ai-cli/"
        fi
    fi
    # .env from sandbox project directory (resolve symlinks)
    local src="$0"
    while [ -L "$src" ]; do
        local dir="$(cd "$(dirname "$src")" && pwd)"
        src="$(readlink "$src")"
        [[ $src != /* ]] && src="$dir/$src"
    done
    local script_path="$(cd "$(dirname "$src")" && pwd)"
    if [ -f "$script_path/.ai-cli.env" ]; then
        if [ "$force" = true ] || [ ! -f "$SANDBOX_CONFIG_DIR/.ai-cli/.env" ]; then
            echo "Syncing .ai-cli.env ..."
            cp "$script_path/.ai-cli.env" "$SANDBOX_CONFIG_DIR/.ai-cli/.env"
        fi
    fi

    # Sync credentials from Keychain (skip in GLM mode - uses ANTHROPIC_AUTH_TOKEN instead)
    if [[ "$GLM_MODE" != "true" ]]; then
        sync_credentials
    fi

    # Mark sandbox as initialized
    touch "$SANDBOX_CONFIG_DIR/.initialized"
    echo "‚úÖ Sandbox config synced to $SANDBOX_CONFIG_DIR"
}

# Determine which Claude config subfolder to use based on mode
if [[ "$GLM_MODE" == "true" ]]; then
    CLAUDE_CONFIG_SUBDIR=".claude-glm"
else
    CLAUDE_CONFIG_SUBDIR=".claude"
fi

# Function to setup GLM config on first GLM run
setup_glm_config() {
    local glm_dir="$SANDBOX_CONFIG_DIR/.claude-glm"

    if [[ ! -d "$glm_dir" ]]; then
        echo "üîß Setting up GLM config (first run)..."

        # Ensure base .claude config exists
        if [[ ! -d "$SANDBOX_CONFIG_DIR/.claude" ]]; then
            echo "Error: No base Claude config found. Run without -g first to sync configs."
            exit 1
        fi

        # Copy from existing Claude config
        cp -r "$SANDBOX_CONFIG_DIR/.claude" "$glm_dir"

        # Remove local npm installation (container uses global install)
        rm -rf "$glm_dir/local"

        # Remove Anthropic-specific credentials (GLM uses different auth)
        # These would cause authentication failures with z.ai proxy
        rm -f "$glm_dir/.credentials.json"
        rm -f "$glm_dir/.credentials"
        rm -f "$glm_dir/statsig_cache.json"
        rm -f "$glm_dir/stats-cache.json"
        rm -f "$glm_dir/.statsig"
        rm -rf "$glm_dir/statsig"
        rm -rf "$glm_dir/telemetry"

        echo "  Cleared Anthropic credentials and telemetry (GLM uses ANTHROPIC_AUTH_TOKEN instead)"

        # Copy GLM-specific settings.json from repo template (always overwrite - Claude's settings are wrong for GLM)
        if [[ -f "$SCRIPT_DIR/configs/glm/settings.json" ]]; then
            cp "$SCRIPT_DIR/configs/glm/settings.json" "$glm_dir/settings.json"
        fi

        # Copy GLM statusline script
        if [[ -f "$SCRIPT_DIR/configs/glm/statusline-robbyrussell.sh" ]]; then
            cp "$SCRIPT_DIR/configs/glm/statusline-robbyrussell.sh" "$glm_dir/"
            chmod +x "$glm_dir/statusline-robbyrussell.sh"
        fi

        # Copy GLM-specific MCP config from repo template (always overwrite)
        if [[ -f "$SCRIPT_DIR/configs/glm/.mcp.json" ]]; then
            cp "$SCRIPT_DIR/configs/glm/.mcp.json" "$glm_dir/"
        fi

        # Copy GLM-specific .claude.json (only if not already exists - preserve user's keys)
        if [[ -f "$SCRIPT_DIR/configs/glm/.claude.json" ]] && [[ ! -f "$SANDBOX_CONFIG_DIR/.claude-glm.json" ]]; then
            cp "$SCRIPT_DIR/configs/glm/.claude.json" "$SANDBOX_CONFIG_DIR/.claude-glm.json"
            echo "  Created GLM-specific .claude.json"
        fi

        echo "‚úÖ GLM config created at $glm_dir"
        echo "‚ö†Ô∏è  Add your ANTHROPIC_AUTH_TOKEN to $glm_dir/settings.json"
        echo "‚ö†Ô∏è  Add your API keys to $SANDBOX_CONFIG_DIR/.claude-glm.json"
    fi
}

# Function to resolve and create a git worktree
resolve_and_create_worktree() {
    local branch_or_path="$1"
    local target_path=""

    # Path Resolution Logic: absolute path or relative to parent
    if [[ "$branch_or_path" == /* ]] || [[ "$branch_or_path" == ../* ]]; then
        target_path="$branch_or_path"
    else
        # Auto-generate path: ../<current-dir>-<branch> (parent directory, prefixed)
        local safe_branch="${branch_or_path//\//-}"
        local current_dir=$(basename "$(pwd)")
        target_path="../${current_dir}-${safe_branch}"
    fi

    # Worktree Creation Logic
    if [[ -d "$target_path" ]]; then
        # Validate it's actually a git worktree
        if ! git worktree list --porcelain | grep -q "^worktree $(cd "$target_path" && pwd)$"; then
            echo "Error: Directory '$target_path' exists but is not a git worktree" >&2
            exit 1
        fi
        # For auto-generated paths, verify the worktree's branch matches the requested branch
        if [[ "$branch_or_path" != /* ]] && [[ "$branch_or_path" != ../* ]]; then
            local actual_branch
            actual_branch=$(cd "$target_path" && git rev-parse --abbrev-ref HEAD)
            if [[ "$actual_branch" != "$branch_or_path" ]]; then
                echo "Error: Directory collision - '$target_path' exists but is on branch '$actual_branch', not '$branch_or_path'" >&2
                exit 1
            fi
        fi
    else
        # Check if branch exists in git (local or any remote)
        if git show-ref --verify --quiet "refs/heads/$branch_or_path" || \
           git show-ref --quiet "refs/remotes/*/$branch_or_path"; then
            # If branch exists, add worktree from it
            if ! git worktree add "$target_path" "$branch_or_path" >&2; then
                echo "Error: Failed to create worktree at $target_path for branch $branch_or_path" >&2
                exit 1
            fi
        else
            # Branch doesn't exist - create it from HEAD
            echo "Creating new branch '$branch_or_path' from HEAD..." >&2
            if ! git worktree add -b "$branch_or_path" "$target_path" HEAD >&2; then
                echo "Error: Failed to create worktree at $target_path with new branch $branch_or_path" >&2
                exit 1
            fi
        fi

        # Copy untracked files (.env*, *.db) from source to NEW worktree only
        local source_dir="$(pwd)"
        local abs_target="$(cd "$target_path" && pwd)"

        # Copy .env* files
        for f in "$source_dir"/.env*; do
            if [[ -f "$f" ]]; then
                if cp "$f" "$abs_target/"; then
                    echo "Copied $(basename "$f") to worktree" >&2
                else
                    echo "Error: Failed to copy $(basename "$f") to worktree" >&2
                    exit 1
                fi
            fi
        done

        # Copy *.db files (top-level only)
        for f in "$source_dir"/*.db; do
            if [[ -f "$f" ]]; then
                if cp "$f" "$abs_target/"; then
                    echo "Copied $(basename "$f") to worktree" >&2
                else
                    echo "Error: Failed to copy $(basename "$f") to worktree" >&2
                    exit 1
                fi
            fi
        done
    fi

    # Return the absolute path
    echo "$(cd "$target_path" && pwd)"
}

cleanup_worktree() {
    local worktree_path="$1"
    local original_dir="$2"

    if [[ -z "$worktree_path" ]] || [[ ! -d "$worktree_path" ]]; then
        echo "No worktree to clean up"
        return 0
    fi

    echo "Cleaning up worktree at $worktree_path..."

    # Check for uncommitted changes before cleanup
    if [[ -n "$(cd "$worktree_path" && git status --porcelain 2>/dev/null)" ]]; then
        echo "‚ö†Ô∏è  Worktree has uncommitted changes - skipping cleanup"
        echo "   Remove manually: git worktree remove \"$worktree_path\" --force"
        return 1
    fi

    # Change back to original directory before removing
    if [[ -n "$original_dir" ]] && [[ -d "$original_dir" ]]; then
        cd "$original_dir"
    fi

    # Remove the worktree (safe - already checked for uncommitted changes)
    if git worktree remove "$worktree_path" 2>/dev/null; then
        echo "‚úÖ Worktree removed successfully"
        return 0
    else
        echo "‚ö†Ô∏è  Failed to remove worktree (may need manual cleanup)"
        return 1
    fi
}

# Handle sync-config mode
if [[ "$SYNC_CONFIG" == true ]]; then
    sync_sandbox_config
    exit 0
fi

# Handle worktree creation and path resolution
WORKTREE_PATH=""
ORIGINAL_DIR="$(pwd)"
if [[ -n "$WORKTREE_BRANCH" ]]; then
    # Resolve script source to absolute path before changing directory
    if [[ "$0" == /* ]]; then
        export SCRIPT_SOURCE="$0"
    elif [[ "$0" == */* ]]; then
        export SCRIPT_SOURCE="$(pwd)/$0"
    else
        export SCRIPT_SOURCE="$(command -v "$0")"
    fi

    WORKTREE_PATH=$(resolve_and_create_worktree "$WORKTREE_BRANCH")
    echo "Using worktree: $WORKTREE_PATH"
    cd "$WORKTREE_PATH" || exit 1
fi

# Dynamic workspace name (sanitized for container path safety)
# Must be after worktree cd to get correct directory name
WORKSPACE_NAME=$(basename "$(pwd)" | tr -cd '[:alnum:]._-' | sed 's/^-//')
WORKSPACE_NAME="${WORKSPACE_NAME:-workspace}"  # fallback if empty
CONTAINER_WORKSPACE="/$WORKSPACE_NAME"

# Handle remove mode
if [[ "$REMOVE_IMAGE" == true ]]; then
    HASH_FILE=".claude-code-sandbox-hash"
    
    if [[ ! -f "$SANDBOX_FILE" ]]; then
        echo "No workspace image to remove (no .claude-code-sandbox file found)"
        exit 0
    fi
    
    IMAGE_TO_REMOVE=$(cat "$SANDBOX_FILE")
    echo "=== Removing Claude Code Sandbox Workspace ==="
    echo "Image to remove: $IMAGE_TO_REMOVE"
    echo ""
    
    # Check if image exists
    if [[ -n "$(docker images -q $IMAGE_TO_REMOVE 2>/dev/null)" ]]; then
        echo "Removing Docker image..."
        if docker rmi "$IMAGE_TO_REMOVE"; then
            echo "‚úÖ Successfully removed image: $IMAGE_TO_REMOVE"
        else
            echo "‚ùå Failed to remove image: $IMAGE_TO_REMOVE"
            echo "   The image might be in use by a running container"
            exit 1
        fi
    else
        echo "‚ö†Ô∏è  Image not found in Docker: $IMAGE_TO_REMOVE"
    fi
    
    # Remove state files
    echo ""
    echo "Removing state files..."
    rm -f "$SANDBOX_FILE"
    echo "‚úÖ Removed .claude-code-sandbox"
    
    if [[ -f "$HASH_FILE" ]]; then
        rm -f "$HASH_FILE"
        echo "‚úÖ Removed .claude-code-sandbox-hash"
    fi
    
    echo ""
    echo "‚úÖ Workspace cleanup complete!"
    exit 0
fi

# Check if tools are specified without --build
if [[ "$BUILD_IMAGE" != true && -n "$INSTALL_TOOLS" ]]; then
    echo "Error: Tools can only be installed with --build flag"
    echo "Usage: $0 --build --install='plugin@version,plugin@version'"
    echo "Examples:"
    echo "  $0 --build --install='python@3.12.8,golang@1.21.5'"
    echo "  $0 --build --install='terraform@1.5.7,maven@3.9.6,kubectl@1.28.0'"
    echo "This ensures tools are installed at build time, not runtime"
    exit 1
fi

# Check for .tool-versions file if --build is specified but no --install
if [[ "$BUILD_IMAGE" == true && -z "$INSTALL_TOOLS" && -f ".tool-versions" ]]; then
    echo "Found .tool-versions file, parsing for tool installation..."
    # Parse .tool-versions file and convert to comma-separated format
    TOOL_VERSIONS_CONTENT=""
    while IFS= read -r line || [[ -n "$line" ]]; do
        # Skip empty lines and comments
        if [[ -n "$line" && ! "$line" =~ ^[[:space:]]*# ]]; then
            # Extract plugin and version
            read -r plugin version <<< "$line"
            if [[ -n "$plugin" && -n "$version" ]]; then
                if [[ -n "$TOOL_VERSIONS_CONTENT" ]]; then
                    TOOL_VERSIONS_CONTENT="$TOOL_VERSIONS_CONTENT,$plugin@$version"
                else
                    TOOL_VERSIONS_CONTENT="$plugin@$version"
                fi
            fi
        fi
    done < ".tool-versions"
    
    if [[ -n "$TOOL_VERSIONS_CONTENT" ]]; then
        INSTALL_TOOLS="$TOOL_VERSIONS_CONTENT"
        echo "Installing tools from .tool-versions: $INSTALL_TOOLS"
    fi
fi

# Get the directory where this script is located (handles symlinks, portable)
SCRIPT_SOURCE="${SCRIPT_SOURCE:-$0}"
while [ -L "$SCRIPT_SOURCE" ]; do
    SCRIPT_DIR="$(cd "$(dirname "$SCRIPT_SOURCE")" && pwd)"
    SCRIPT_SOURCE="$(readlink "$SCRIPT_SOURCE")"
    [[ $SCRIPT_SOURCE != /* ]] && SCRIPT_SOURCE="$SCRIPT_DIR/$SCRIPT_SOURCE"
done
SCRIPT_DIR="$(cd "$(dirname "$SCRIPT_SOURCE")" && pwd)"

# Check if base image needs to be built/updated
BASE_IMAGE_EXISTS="$(docker images -q $BASE_IMAGE_NAME 2> /dev/null)"
if [[ -z "$BASE_IMAGE_EXISTS" ]] || [[ "$REBUILD_IMAGE" == true ]]; then
    if [[ "$REBUILD_IMAGE" == true ]]; then
        echo "Rebuilding shared base image without cache: $BASE_IMAGE_NAME"
        docker build --no-cache -f "$SCRIPT_DIR/Dockerfile" --target base -t $BASE_IMAGE_NAME "$SCRIPT_DIR"
    else
        echo "Building shared base image: $BASE_IMAGE_NAME"
        docker build -f "$SCRIPT_DIR/Dockerfile" --target base -t $BASE_IMAGE_NAME "$SCRIPT_DIR"
    fi
fi

# Generate content hash for incremental build detection
CONTENT_HASH=$(get_content_hash "$SCRIPT_DIR")
HASH_FILE=".claude-code-sandbox-hash"

# Check if rebuild is needed
NEEDS_REBUILD=false
if [[ "$BUILD_IMAGE" == true ]]; then
    NEEDS_REBUILD=true
    echo "Forced rebuild requested"
elif [[ -n "$INSTALL_TOOLS" ]]; then
    NEEDS_REBUILD=true
    echo "Tools specified, rebuild needed"
elif [[ "$(docker images -q $IMAGE_NAME 2> /dev/null)" == "" ]]; then
    NEEDS_REBUILD=true
    echo "Image doesn't exist, rebuild needed"
    # Auto-detect .tool-versions for new workspaces
    if [[ -f ".tool-versions" && -z "$INSTALL_TOOLS" ]]; then
        echo "Found .tool-versions file, parsing for tool installation..."
        # Parse .tool-versions file and convert to comma-separated format
        TOOL_VERSIONS_CONTENT=""
        while IFS= read -r line || [[ -n "$line" ]]; do
            # Skip empty lines and comments
            if [[ -n "$line" && ! "$line" =~ ^[[:space:]]*# ]]; then
                # Extract plugin and version
                read -r plugin version <<< "$line"
                if [[ -n "$plugin" && -n "$version" ]]; then
                    if [[ -n "$TOOL_VERSIONS_CONTENT" ]]; then
                        TOOL_VERSIONS_CONTENT="$TOOL_VERSIONS_CONTENT,$plugin@$version"
                    else
                        TOOL_VERSIONS_CONTENT="$plugin@$version"
                    fi
                fi
            fi
        done < ".tool-versions"
        
        if [[ -n "$TOOL_VERSIONS_CONTENT" ]]; then
            INSTALL_TOOLS="$TOOL_VERSIONS_CONTENT"
            echo "Installing tools from .tool-versions: $INSTALL_TOOLS"
        fi
    fi
elif [[ ! -f "$HASH_FILE" ]] || [[ "$(cat "$HASH_FILE" 2>/dev/null)" != "$CONTENT_HASH" ]]; then
    NEEDS_REBUILD=true
    echo "Configuration changed, incremental rebuild needed"
fi

# Build workspace-specific image if needed
if [[ "$NEEDS_REBUILD" == true ]]; then
    echo "Building workspace image: $IMAGE_NAME"
    BUILD_ARGS="--build-arg BASE_IMAGE=$BASE_IMAGE_NAME"
    if [ -n "$INSTALL_TOOLS" ]; then
        BUILD_ARGS="$BUILD_ARGS --build-arg INSTALL_TOOLS='$INSTALL_TOOLS'"
        echo "Installing tools: $INSTALL_TOOLS"
    fi
    
    # Add --no-cache flag if rebuild is requested
    if [[ "$REBUILD_IMAGE" == true ]]; then
        echo "Forcing full rebuild without cache..."
        BUILD_ARGS="--no-cache $BUILD_ARGS"
    fi
    
    docker build $BUILD_ARGS -f "$SCRIPT_DIR/Dockerfile" --target workspace -t $IMAGE_NAME "$SCRIPT_DIR"
    
    # Save content hash for future incremental builds
    echo "$CONTENT_HASH" > "$HASH_FILE"
else
    echo "No changes detected, using existing image: $IMAGE_NAME"
fi


# Check for Claude configuration directories
CLAUDE_CONFIG_ARGS=()

# Auto-sync sandbox config on first run only (check for .initialized marker)
if [ ! -f "$SANDBOX_CONFIG_DIR/.initialized" ]; then
    echo "First run detected - syncing configs to sandbox..."
    sync_sandbox_config false  # false = don't overwrite existing configs
fi

# Setup GLM config on first GLM run
if [[ "$GLM_MODE" == "true" ]]; then
    setup_glm_config
else
    # Auto-sync credentials on every start (writes only if changed)
    sync_credentials
fi

# Mount sandbox configs (persistent, protects host files)
# Use CLAUDE_CONFIG_SUBDIR to support both Claude and GLM modes
if [ -d "$SANDBOX_CONFIG_DIR/$CLAUDE_CONFIG_SUBDIR" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/$CLAUDE_CONFIG_SUBDIR:/home/node/.claude")
    echo "Using Claude config: $SANDBOX_CONFIG_DIR/$CLAUDE_CONFIG_SUBDIR"
fi

if [ -d "$SANDBOX_CONFIG_DIR/.config/claude" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/.config/claude:/home/node/.config/claude")
    echo "Using sandbox config: .config/claude"
fi

if [ -d "$SANDBOX_CONFIG_DIR/.anthropic" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/.anthropic:/home/node/.anthropic")
    echo "Using sandbox config: .anthropic"
fi

# Linux-specific cache directory (isolates platform-specific compiled artifacts)
LINUX_CACHE_DIR="$HOME/.claude-sandbox-cache"
mkdir -p "$LINUX_CACHE_DIR" || { echo "Error: Failed to create cache directory $LINUX_CACHE_DIR" >&2; exit 1; }

# Helper: mount a Linux-isolated cache directory
add_linux_cache() {
    local name="$1"
    mkdir -p "$LINUX_CACHE_DIR/$name" || { echo "Error: Failed to create cache directory $LINUX_CACHE_DIR/$name" >&2; exit 1; }
    CLAUDE_CONFIG_ARGS+=(-v "$LINUX_CACHE_DIR/$name:/home/node/$name")
    echo "Using Linux cache: $name"
}

# Mount Linux-specific caches (auto-created, isolated from macOS host caches)
# These contain platform-specific compiled artifacts that don't cross-compile well
add_linux_cache .cargo      # Rust compiled deps
add_linux_cache go          # Go binaries
add_linux_cache .bun        # Bun native binaries
add_linux_cache .cache      # pip wheels, yarn, pnpm, go-build, deno, composer
add_linux_cache .pnpm-store # pnpm native deps
add_linux_cache .gem        # Ruby native extensions
add_linux_cache .bundle     # Ruby bundler native deps
add_linux_cache .m2         # Maven JARs
add_linux_cache .gradle     # Gradle cache
add_linux_cache .npm        # npm cache
add_linux_cache .nuget      # NuGet packages
add_linux_cache .local/share/uv  # uv/uvx tools and cache

# Mount sandbox .claude.json (use GLM-specific version in GLM mode)
if [[ "$GLM_MODE" == "true" ]] && [ -f "$SANDBOX_CONFIG_DIR/.claude-glm.json" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/.claude-glm.json:/home/node/.claude.json")
    echo "Using GLM config: .claude-glm.json"
elif [ -f "$SANDBOX_CONFIG_DIR/.claude.json" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/.claude.json:/home/node/.claude.json")
    echo "Using sandbox config: .claude.json"
fi

# Mount sandbox opencode config
if [ -d "$SANDBOX_CONFIG_DIR/.config/opencode" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/.config/opencode:/home/node/.config/opencode")
    echo "Using sandbox config: .config/opencode"
fi

# Mount sandbox opencode data
if [ -d "$SANDBOX_CONFIG_DIR/.local/share/opencode" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/.local/share/opencode:/home/node/.local/share/opencode")
    echo "Using sandbox config: .local/share/opencode"
fi

# Mount sandbox Qwen Code config
if [ -d "$SANDBOX_CONFIG_DIR/.qwen" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/.qwen:/home/node/.qwen")
    echo "Using sandbox config: .qwen"
fi

# Mount sandbox Codex config (OpenAI Codex CLI)
if [ -d "$SANDBOX_CONFIG_DIR/.codex" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/.codex:/home/node/.codex")
    echo "Using sandbox config: .codex"
fi

# Mount sandbox Gemini config (Google Gemini CLI)
if [ -d "$SANDBOX_CONFIG_DIR/.gemini" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/.gemini:/home/node/.gemini")
    echo "Using sandbox config: .gemini"
fi

# Mount AI-CLI config files (not the whole dir - repo is cloned there)
if [ -f "$SANDBOX_CONFIG_DIR/.ai-cli/config.json" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/.ai-cli/config.json:/home/node/.ai-cli/config.json")
    echo "Using sandbox config: .ai-cli/config.json"
fi
if [ -f "$SANDBOX_CONFIG_DIR/.ai-cli/.env" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/.ai-cli/.env:/home/node/.ai-cli/.env")
    echo "Using sandbox config: .ai-cli/.env"
fi

# Mount sandbox opencode cache (overrides host ~/.cache/opencode)
if [ -d "$SANDBOX_CONFIG_DIR/.cache/opencode" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/.cache/opencode:/home/node/.cache/opencode")
    echo "Using sandbox config: .cache/opencode"
fi

# Mount sandbox opencode state (model selection, favorites, history)
if [ -d "$SANDBOX_CONFIG_DIR/.local/state/opencode" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$SANDBOX_CONFIG_DIR/.local/state/opencode:/home/node/.local/state/opencode")
    echo "Using sandbox config: .local/state/opencode"
fi

# Check for basic-memory directory (basic-memory MCP server)
if [ -d "$HOME/basic-memory" ]; then
    CLAUDE_CONFIG_ARGS+=(-v "$HOME/basic-memory:/home/node/basic-memory")
    echo "Mounting basic-memory MCP server from $HOME/basic-memory"
fi

# Check for .beads directory (beads task management)
# Use array to properly handle paths with spaces
BEADS_MOUNT_ARGS=()
if [ -d "$(pwd)/.beads" ]; then
    BEADS_MOUNT_ARGS=(-v "$(pwd)/.beads:$CONTAINER_WORKSPACE/.beads")
    echo "Mounting beads task directory from $(pwd)/.beads"
fi

# Handle git worktree: mount main repo's .git directory
# Worktrees have a .git FILE (not dir) pointing to main repo
# Example: gitdir: /path/to/repo/.git/worktrees/branch-name
#          ‚Üí dirname twice ‚Üí /path/to/repo/.git
GIT_WORKTREE_ARGS=()
if [ -f "$(pwd)/.git" ]; then
    GITDIR_PATH=$(sed -n 's/^gitdir: //p' "$(pwd)/.git")
    if [ -n "$GITDIR_PATH" ]; then
        # .git/worktrees/<name> ‚Üí .git/worktrees ‚Üí .git
        MAIN_GIT_DIR=$(dirname "$(dirname "$GITDIR_PATH")")
        if [ -d "$MAIN_GIT_DIR" ]; then
            GIT_WORKTREE_ARGS=(-v "$MAIN_GIT_DIR:$MAIN_GIT_DIR")
            echo "Mounting main repo .git for worktree: $MAIN_GIT_DIR"
        fi
    fi
fi

# Add Docker socket mounting if Docker flag is enabled
DOCKER_MOUNT=""
if [ "$DOCKER_ENABLED" = true ]; then
    if [ -S "/var/run/docker.sock" ]; then
        DOCKER_MOUNT="-v /var/run/docker.sock:/var/run/docker.sock"
        echo "Mounting Docker socket for container access"
    else
        echo "Warning: Docker socket not found at /var/run/docker.sock"
        echo "Docker commands may not work inside the container"
    fi
fi

if [ ${#CLAUDE_CONFIG_ARGS[@]} -eq 0 ]; then
    echo "Warning: No configuration directories found"
fi

# Setup cleanup trap for worktree before running docker (must be before blocking docker run)
if [[ "$CLEANUP_WORKTREE" == true && -n "$WORKTREE_PATH" ]]; then
    trap "cleanup_worktree \"$WORKTREE_PATH\" \"$ORIGINAL_DIR\"" EXIT
    echo "Worktree cleanup will run on exit"
fi

# Check if we have a TTY or if non-interactive mode is requested
if [ -t 0 ] || [ "$NON_INTERACTIVE" = true ]; then
    if [ "$SHELL_MODE" = true ]; then
        # Run interactive shell instead of Claude Code
        docker run -it --rm \
            --user node \
            -v "$(pwd):$CONTAINER_WORKSPACE" \
            -w "$CONTAINER_WORKSPACE" \
            "${BEADS_MOUNT_ARGS[@]}" \
            "${GIT_WORKTREE_ARGS[@]}" \
            "${CLAUDE_CONFIG_ARGS[@]}" \
            $DOCKER_MOUNT \
            -e TERM="$TERM" \
            -e CONTAINER_WORKSPACE="$CONTAINER_WORKSPACE" \
            --init \
            "$IMAGE_NAME" \
            bash -c "source ~/.asdf/asdf.sh 2>/dev/null || true; $(generate_venv_setup); exec bash --rcfile <(echo 'source ~/.bashrc 2>/dev/null; [ -f \"$CONTAINER_WORKSPACE/.venv-linux/bin/activate\" ] && source \"$CONTAINER_WORKSPACE/.venv-linux/bin/activate\"')"
    elif [ "$NON_INTERACTIVE" = true ]; then
        # Run non-interactive command with asdf support
        docker run -it --rm \
            --user node \
            -v "$(pwd):$CONTAINER_WORKSPACE" \
            -w "$CONTAINER_WORKSPACE" \
            "${BEADS_MOUNT_ARGS[@]}" \
            "${GIT_WORKTREE_ARGS[@]}" \
            "${CLAUDE_CONFIG_ARGS[@]}" \
            $DOCKER_MOUNT \
            -e TERM="$TERM" \
            -e CONTAINER_WORKSPACE="$CONTAINER_WORKSPACE" \
            --init \
            "$IMAGE_NAME" \
            bash -c "export PATH=\"\$HOME/.npm-global/bin:\$HOME/.local/bin:\$PATH\"; source ~/.asdf/asdf.sh 2>/dev/null || true; $(generate_venv_setup); $*"
    else
        # Run the container with current directory mounted as workspace
        docker run -it --rm \
            --user node \
            -v "$(pwd):$CONTAINER_WORKSPACE" \
            -w "$CONTAINER_WORKSPACE" \
            "${BEADS_MOUNT_ARGS[@]}" \
            "${GIT_WORKTREE_ARGS[@]}" \
            "${CLAUDE_CONFIG_ARGS[@]}" \
            $DOCKER_MOUNT \
            -e TERM="$TERM" \
            -e CONTAINER_WORKSPACE="$CONTAINER_WORKSPACE" \
            --init \
            "$IMAGE_NAME" \
            bash -c "export PATH=\"\$HOME/.npm-global/bin:\$HOME/.local/bin:\$PATH\"; source ~/.asdf/asdf.sh 2>/dev/null || true; $(generate_venv_setup); if ! command -v node >/dev/null 2>&1; then echo 'No Node.js found, installing LTS...'; asdf plugin add nodejs 2>/dev/null || true; asdf install nodejs lts; asdf global nodejs lts; fi; claude-wrapper --dangerously-skip-permissions $*"
    fi
else
    echo "Error: This script requires an interactive terminal (TTY)."
    echo "Please run this script from a regular terminal, not from within another program."
    echo "The script is designed to launch an interactive Claude session."
    echo "Use -n or --non-interactive to run commands without TTY."
    exit 1
fi